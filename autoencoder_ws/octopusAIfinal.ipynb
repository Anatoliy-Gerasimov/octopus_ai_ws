{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=6>Part 1. Autoencoder development</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./octopusAI/autoencoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Reshape, Input\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./octopusAI/example0.png) ![title](./octopusAI/example1.png) ![title](./octopusAI/example2.png)\n",
    "![title](./octopusAI/example3.png) ![title](./octopusAI/example4.png) ![title](./octopusAI/example5.png)\n",
    "![title](./octopusAI/example6.png) ![title](./octopusAI/example7.png) ![title](./octopusAI/example8.png)\n",
    "![title](./octopusAI/example9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1)) \n",
    "\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Reshape((392,))(x) \n",
    "\n",
    "encoded = Dense(32,activation='relu')(x)\n",
    "\n",
    "x = Dense(392,activation='relu')(encoded)\n",
    "x = Reshape((7,7,8))(x) \n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='relu', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.reshape(X_train, (len(X_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "X_test = np.reshape(X_test, (len(X_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                12576     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 392)               12936     \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 28,729\n",
      "Trainable params: 28,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"./octopusAI/model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 2107.2111 - val_loss: 1166.1915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1166.19154, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 1049.2278 - val_loss: 924.0068\n",
      "\n",
      "Epoch 00002: val_loss improved from 1166.19154 to 924.00677, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 888.8762 - val_loss: 821.0028\n",
      "\n",
      "Epoch 00003: val_loss improved from 924.00677 to 821.00285, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 811.9210 - val_loss: 761.4447\n",
      "\n",
      "Epoch 00004: val_loss improved from 821.00285 to 761.44475, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 762.8212 - val_loss: 722.1033\n",
      "\n",
      "Epoch 00005: val_loss improved from 761.44475 to 722.10327, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 729.1796 - val_loss: 716.2815\n",
      "\n",
      "Epoch 00006: val_loss improved from 722.10327 to 716.28147, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 702.6942 - val_loss: 672.0837\n",
      "\n",
      "Epoch 00007: val_loss improved from 716.28147 to 672.08369, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 681.9568 - val_loss: 660.0619\n",
      "\n",
      "Epoch 00008: val_loss improved from 672.08369 to 660.06193, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 665.9571 - val_loss: 645.1728\n",
      "\n",
      "Epoch 00009: val_loss improved from 660.06193 to 645.17281, saving model to ./octopusAI/model.hdf5\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 62s 1ms/step - loss: 654.8143 - val_loss: 641.8311\n",
      "\n",
      "Epoch 00010: val_loss improved from 645.17281 to 641.83112, saving model to ./octopusAI/model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c68551940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = X_test[100]\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp3d = np.concatenate([inp, inp, inp], axis = 2)\n",
    "inp3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp4pred = inp.reshape((1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "misc.imsave('./octopusAI/real100.png', inp3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "(28, 28, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "res = autoencoder.predict(inp4pred)\n",
    "print(res.shape)\n",
    "res = res.reshape((28,28,1))\n",
    "res = np.concatenate([res, res, res], axis = 2)\n",
    "print(res.shape)\n",
    "misc.imsave('./octopusAI/pred100.png', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./octopusAI/real100.png)\n",
    "![title](./octopusAI/pred100.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_neck_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[6].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=6>Part 2. Dataset for clustering</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bottle_neck_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST = bottle_neck_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "number_of_classes = 10\n",
    "target_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "target_test = np_utils.to_categorical(y_test, number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./octopusAI/convolutional_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = Sequential()\n",
    "conv_net.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "conv_net.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "conv_net.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "conv_net.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "conv_net.add(Dropout(0.2))\n",
    "conv_net.add(Flatten())\n",
    "conv_net.add(Dense(128, activation='relu'))\n",
    "conv_net.add(Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 41s 685us/step - loss: 3.2204 - acc: 0.7469 - val_loss: 0.0967 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96960, saving model to ./octopusAI/convolutional_model.hdf5\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 41s 676us/step - loss: 0.1167 - acc: 0.9647 - val_loss: 0.0644 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96960 to 0.97950, saving model to ./octopusAI/convolutional_model.hdf5\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 40s 669us/step - loss: 0.0761 - acc: 0.9758 - val_loss: 0.0525 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.97950 to 0.98210, saving model to ./octopusAI/convolutional_model.hdf5\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 40s 671us/step - loss: 0.0634 - acc: 0.9799 - val_loss: 0.0473 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98210 to 0.98420, saving model to ./octopusAI/convolutional_model.hdf5\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 40s 674us/step - loss: 0.0507 - acc: 0.9844 - val_loss: 0.0487 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98420 to 0.98570, saving model to ./octopusAI/convolutional_model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c6055df60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net.fit(X_train, target_train, validation_data=(X_test, target_test), epochs=5, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./octopusAI/feedforward_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_net = Sequential()\n",
    "feed_net.add(Dense(64, input_shape=(32,), activation='relu'))\n",
    "feed_net.add(Dropout(0.2))\n",
    "feed_net.add(Dense(64, activation='relu'))\n",
    "feed_net.add(Dropout(0.2))\n",
    "feed_net.add(Dense(64, activation='relu'))\n",
    "feed_net.add(Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_net.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 12.2442 - acc: 0.2344 - val_loss: 10.5002 - val_acc: 0.3444\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.34440, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 8.9163 - acc: 0.4355 - val_loss: 6.0879 - val_acc: 0.6123\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.34440 to 0.61230, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 5.8534 - acc: 0.6169 - val_loss: 3.3392 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.61230 to 0.77960, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 4.0499 - acc: 0.7219 - val_loss: 2.6451 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77960 to 0.81900, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 3.3545 - acc: 0.7531 - val_loss: 1.2202 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.81900 to 0.88120, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 1.6860 - acc: 0.7854 - val_loss: 0.5820 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.9754 - acc: 0.7694 - val_loss: 0.4554 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.7689 - acc: 0.7982 - val_loss: 0.3927 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88120 to 0.90010, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.6573 - acc: 0.8180 - val_loss: 0.3345 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.90010 to 0.90840, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.5713 - acc: 0.8380 - val_loss: 0.3112 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.90840 to 0.91660, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.5201 - acc: 0.8502 - val_loss: 0.2890 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.91660 to 0.91940, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.4768 - acc: 0.8610 - val_loss: 0.2638 - val_acc: 0.9232\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.91940 to 0.92320, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.4425 - acc: 0.8699 - val_loss: 0.2407 - val_acc: 0.9330\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.92320 to 0.93300, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.4141 - acc: 0.8768 - val_loss: 0.2370 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3925 - acc: 0.8839 - val_loss: 0.2155 - val_acc: 0.9386\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.93300 to 0.93860, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3724 - acc: 0.8857 - val_loss: 0.2112 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3509 - acc: 0.8930 - val_loss: 0.2008 - val_acc: 0.9407\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.93860 to 0.94070, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3436 - acc: 0.8960 - val_loss: 0.1898 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.94070 to 0.94300, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3251 - acc: 0.9002 - val_loss: 0.1824 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.94300 to 0.94640, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.3174 - acc: 0.9035 - val_loss: 0.1827 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.3011 - acc: 0.9070 - val_loss: 0.1781 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.9102 - val_loss: 0.1685 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.94640 to 0.94950, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.2875 - acc: 0.9112 - val_loss: 0.1630 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.94950 to 0.95060, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.2745 - acc: 0.9163 - val_loss: 0.1594 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.95060 to 0.95360, saving model to ./octopusAI/feedforward_model.hdf5\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.2704 - acc: 0.9176 - val_loss: 0.1521 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.95360 to 0.95370, saving model to ./octopusAI/feedforward_model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c2f4559b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_net.fit(X, target_train, validation_data=(X_TEST, target_test), epochs=25, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net.load_weights(\"./octopusAI/convolutional_model.hdf5\")\n",
    "feed_net.load_weights(\"./octopusAI/feedforward_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "res_conv = conv_net.predict(X_test)\n",
    "res_feedforward = feed_net.predict(X_TEST)\n",
    "\n",
    "confusion_conv = confusion_matrix(target_test.argmax(axis=1), res_conv.argmax(axis=1))\n",
    "confusion_feedforward = confusion_matrix(target_test.argmax(axis=1), res_feedforward.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 970    0    4    0    1    0    3    1    0    1]\n",
      " [   0 1128    1    1    0    0    4    0    1    0]\n",
      " [   1    0 1024    0    0    1    0    5    1    0]\n",
      " [   0    0    1 1005    0    1    0    1    2    0]\n",
      " [   0    0    0    0  974    0    3    0    0    5]\n",
      " [   1    0    0   13    0  876    1    1    0    0]\n",
      " [   3    1    0    0    2    4  945    0    1    2]\n",
      " [   0    4    4    3    0    0    0 1013    1    3]\n",
      " [   7    0    7    5    2    7    0    5  937    4]\n",
      " [   2    2    1    2    9    3    0    3    2  985]]\n",
      "========================================================\n",
      "[[ 963    0    0    1    0    2    7    1    5    1]\n",
      " [   0 1114    7    2    2    1    3    0    6    0]\n",
      " [  11    0  975   14    5    0    5    9   13    0]\n",
      " [   2    0    4  959    0   12    1    5   21    6]\n",
      " [   0    0    1    0  943    0    4    3    1   30]\n",
      " [   3    1    0   24    2  836   12    1   10    3]\n",
      " [   4    3    0    2    2    4  939    0    4    0]\n",
      " [   2    5   12    2    5    0    0  951    0   51]\n",
      " [   5    0    4   22    5   10    3    2  903   20]\n",
      " [   7    4    0   15   18    0    0    4    7  954]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_conv)\n",
    "print('========================================================')\n",
    "print(confusion_feedforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=6>Part 3. Clustering</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=15).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = kmeans.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64  72  32   1   3 695  13   1  97   2]\n",
      " [  9   2   5   0   2   0 333   3   4 777]\n",
      " [ 13 251  33   8 535  11  43   9  86  43]\n",
      " [108 203   5  12 137  22  13  17 468  25]\n",
      " [ 80  51  33 114   4   0  97 583   0  20]\n",
      " [265  67  13  29  14  35 154  32 271  12]\n",
      " [ 13 100 753   0   2  12  37   8   3  30]\n",
      " [ 26   9   0 663   5   1  46 235   0  43]\n",
      " [447 155   9  29   8   5  69  47 176  29]\n",
      " [ 51  21   0 323   1   5  10 575   7  16]]\n"
     ]
    }
   ],
   "source": [
    "kmeans_confusion = np.zeros((10,10))\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    kmeans_confusion[y_test[i], xt[i]] +=1\n",
    "    \n",
    "kmeans_confusion = kmeans_confusion.astype(int)\n",
    "\n",
    "print(kmeans_confusion)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
